---
title: 'Relationships between words: n-grams and correlations'
author: "J.H AHN"
date: '2022 1 25 '
output: 
  html_document:
    toc: TRUE
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Relationships between words: n-grams and correlations {#ngrams}

지금까지 단어를 개별 단위로 간주하여 감정 또는 문서와의 관계를 고려했다. 그러나 많은 텍스트 분석은 어떤 단어가 다른 단어와 붙어서 사용되는 경향이 있기도 한고 동일한 문서 내에서 함께 사용(co-occur)되는 경향이 있기도 하다. 이런 단어 간의 관계를 기반으로 한 분석 방법을 알아본다. 

이 챕터에서는 텍스트 데이터 셋의 단어 간의 관계를 계산하고 시각화 하기 위하여 `tidytext`package가 제공하는 몇가지 방법을 살펴볼 것이다. 여기에는 개별 단어가 아닌 인접한 단어 쌍을 토큰화 하는 `token = "ngrams"` argument가 사용된다. 또한 ggplot2를 확장하여 네트워크 플롯을 그려주는 [ggraph](https://github.com/thomasp85/ggraph)와 tidy data frame에서 각 쌍의 상관관계 및 거리를 계산해 주는 [widyr](https://github.com/dgrtwo/widyr)를 사용해 볼 것이다.  

## Tokenizing by n-gram

`unnest_tokens`을 사용하여 단어 또는 문장으로 토큰화 하였는데 이러한 방법은 지금까지 해 온 감정 및 빈도 분석 시 유용하다. 하지만 함수를 사용하여 **n-grams**으로 불리는 연속적인 단어 시퀀스로 토큰화 할 수도 있다. 단어 X 뒤에 단어 Y가 얼마나 자주 나오는지 확인함으로써 이들 사이의 관계 모델을 구축할 수 있다.  

`unnest_tokens()`에 `token = "ngrams"`로 설정하고 `n` 각 n-gram에서 캡처하려는 단어 수로 설정하면 된다. `n`을 2로 설정하면 "bigrams"으로 불리는 두 개의 연속 단어 쌍을 토큰으로 만든다.  

```{r austen_bigrams}
library(dplyr)
library(tidytext)
library(janeaustenr)

austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams
```

이 데이터 구조는 여전히 tidy text format을 유지하고 있다. 하나의 행에 하나의 토큰으로 구성되어 있지만 ('book'과 같은 추가 메타데이터는 보존되어 있음) 각 토큰이 bigram으로 나타난 것이 차이이다.  

이런 bigram overlap이 발생하는데 주의해야 합니다: "sense and"가 하나의 토큰으로 나타나는데 "and sensibility"도 또 하나의 토큰으로 나타난다.  

### Counting and filtering n-grams

tidy tools은 n-gram분석에도 동일하게 사용된다. dplyr의 `count()`를 사용하여 bigram의 갯수를 확인할 수 있다.  

```{r, dependson = "austen_bigrams"}
austen_bigrams %>%
  count(bigram, sort = TRUE)
```

예상한대로 가장 많은 빈도로 나타나는 bigram의 대부분은 'of' 및 'to be'와 같은 의미없는 불용어(stopwords)이다. 구분문자(delimiter)를 기초로 열을 여러 개로 분할하는 `seperate()`를 사횽하면 "단어1"과 "단어2"라는 두 개의 열로 분리할 수 있드며 이 중 하나가 불용어일 경우 제거할 수 있다.  

```{r bigram_counts, dependson = "austen_bigrams"}
library(tidyr)

bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

제인 오스틴의 책에서 이름(이름과 성 또는 인사말)이 가장 흔한 단어 쌍임을 알 수 있다.  

다른 분석을 수행할 때는 단어를 재결합하여 작업할 수 있다. `tidyr`package의 `unite()`는 `seperate()`과 반대 역할을 하는 함수이며 두 개의 열을 하나로 다시 만들 수 있다. 즉 `seperate()`, `filter()`, `count()`, `unite()`를 사용하면 stopword를 포함하지 않는 가장 일반적인 bigram을 찾을 수 있다.  

```{r bigrams_united, dependson = "bigram_counts"}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```

다른 분석을 할 때는 가장 흔하게 등장하는 trigram을 찾아야 할 수도 있다. 이 때는 `n = 3`으로 설정하면 된다.  

```{r}
austen_books() %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)
```

### Analyzing bigrams

one-bigram-per-row format은 텍스트의 탐색적 분석에 유용하다. 간단한 예로 각 책에서 가장 흔하게 나온 "streets"을 찾아야 할 수도 있다.  

```{r bigrams_filtered_street, dependson = "bigram_counts"}
bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)
```

bigram은 개별 단어와 같은 방식으로 취급할 수 있다. 예를 들어 오스틴 소설에서 bigram의 tf-idf(3장 참조)를 확인할 수 있다. 이러한 tf-idf값은 단어와 같은 방식으로 각 책 내에서 시각화 할 수 있다.  

```{r bigram_tf_idf, dependson = "bigram_counts"}
bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```

```{r bigramtfidf, dependson = "bigram_tf_idf", echo = FALSE, fig.width=6, fig.height=8, fig.cap = "Bigrams with the highest tf-idf from each Jane Austen novel"}
library(ggplot2)

bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(tf_idf, bigram, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ book, ncol = 2, scales = "free") +
  labs(x = "tf-idf of bigram", y = NULL)
```

3장에서 확인한 것과 같이 오스틴 소설을 구별하는 bigram은 거의 이름이다. 오만과 편견의 "replied elizabeth" 또는 Emma의 "cired emma"와 같은 일반적인 동사와 이름이 합쳐진 bigram을 볼 수 있다. (어디서?? 안 보이는데??) 

개별 단어보다 birgram의 tf-idf로 분석하는데 다음과 같은 장단점이 있다. 연속된 단어 쌍은 단일 단어를 계산할 때 존재하지 않는 구조를 포착하고 토큰을 더 이해하기 쉽게 만드는 컨텍스트를 가질 수 있다. (예를 들어, "pulteney street", in Northanger Abbey, is more informative than "pulteney"). 그러나 bigram당 갯수 또한 *희소(sparser)*하다. 일반적인 두 단어의 쌍은 단일 단어보다 더 드물게 나타나기 때문에 bigram은 매우 큰 텍스트 데이터 세트가 있는 경우에 유용하다.  

### Using bigrams to provide context in sentiment analysis

2장의 감정 분석 접근 방식은 참조 어휘에 따라 단순히 긍적적이거나 부정적인 단어의 출현을 계산했다. 이 접근 방식의 문제점 중 하나는 단어의 컨텍스트가 단어 자체보다 중요한 경우를 놓친다는 것이다. 예를 들어 "I'm not **happy** and I don't **like** it!"과 같은 문장에서도 "happy" 및 "like"라는 단어는 긍정적인 것으로 간주된다.  

이제 데이터를 bigram으로 구성했으므로 단어 앞에 "not"과 같은 단어가 오는 빈도 수를 확인해 본다.  

```{r dependson = "bigrams_separated"}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```

bigram data에 대한 감정 분석을 수행하여 감정 관련 단어 앞에 "not" 또는 기타 부정 단어가 오는 빈도를 조사할 수 있다. 우리는 이것을 사용하여 감정 점수를 무시하거나 반대로 받아들일 수 있다.  

감정 분석을 위해 AFINN 어휘를 사용한다. 각 단어에 대한 숫자 감정 값을 제공하며 감정을 방향은 양수 혹은 음수로 표현된다.  

```{r eval=TRUE}
AFINN <- get_sentiments("afinn")

AFINN
```

```{r AFINN_ngrams, echo=FALSE}
#load("data/afinn.rda")
#AFINN <- afinn

#AFINN
```


그 다음 "not"이 앞에 오고 감정과 관련된 가장 빈번한 단어를 확인할 수 있다.  

```{r not_words, dependson = c("austen_bigrams", "AFINN_ngrams")}
not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)

not_words
```

예를 들어 "not"뒤에 오는 가장 일반적인 단어는 "like"였으며 like는 +2점인 단어이다.  

어떤 단어가 "잘못된" 방향으로 가장 많이 기여했는지 확인할 수도 있다. 이를 계산하기 위해서 해당 값에 나타나는 횟수를 곱하면 된다. (예를 들어 +3의 값을 가지는 단어가 10번 있었다면 이는 +1점 짜리 단어가 30번 있는 것과 같은 효과를 보인다.) bar plot을 이용하여 이를 시각화 한다.  

```{r notwordsplot, dependson = "not_words", fig.cap = "Words preceded by 'not' that had the greatest contribution to sentiment values, in either a positive or negative direction"}

library(ggplot2)

not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"")
```

bigram 중 "not like" 및 "not help"가 압도적으로 큰 영향을 미쳤음이 확인되었다. 즉 텍스트가 실제보다 훨씬 더 긍정적으로 평가되었다는 뜻이다. 반대로 "not afraid"나 "not fail"과 같이 텍스트가 실제보다 덜 부정적으로 만든 bigram도 존재한다는 것을 알 수 있다.  

"Not"은 다음 단어의 뜻을 바꾸는 유일한 단어가 아니다. 따라오는 단어를 부정하는 대표적인 4개 단어(혹은 그 이상)를 선택하여 동일하게 분석을 해 볼 수 있다.  

```{r negated_words, dependson = "bigrams_separated"}
negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)

negated_words
```

그런 다음 각 특정 부정어 뒤에 오는 가장 흔한 단어가 무엇인지 시각화 할 수 있다. (그림 4.3 참조). "not like"와 "not help"가 가장 일반적인 두 가지 예이지만 "no great"와 "never loved."와 같은 bigram도 있음을 확인할 수 있다. 우리는 이러한 부정 뒤에 오는 각 단어의 AFINN값을 반대로 만들기 위해 2장에서 배운 접근 방식을 응용해 볼 수 있다. 아래 코드는 bigram을 이용해 문맥을 이해하는 텍스트 마이닝 방법 중 하나의 예를 보여준다.  

```{r negatedwords, dependson = "negated_words", fig.width=7, fig.height=7, echo = FALSE, fig.cap = "Most common positive or negative words to follow negations such as 'never', 'no', 'not', and 'without'"}
negated_words %>%
  mutate(contribution = n * value,
         word2 = reorder(paste(word2, word1, sep = "__"), contribution)) %>%
  group_by(word1) %>%
  slice_max(abs(contribution), n = 12, with_ties = FALSE) %>%
  ggplot(aes(word2, contribution, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free") +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab("Words preceded by negation term") +
  ylab("Sentiment value * # of occurrences") +
  coord_flip()
```

### Visualizing a network of bigrams with ggraph

한 번에 상위 몇 개의 단어만 표시하는 것이 아니라 모든 단어 간의 관계를 동시에 시각화 하는 것도 가능하다. 하나의 일반적인 시각화 방법으로 네트워크 그래프을 들 수 있다. 여기서 그래프 시각화의 의미보다는 연결된 노드의 조합으로 봐야 한다. 그래프는 세 가지 변수를 사용하여 tidy object를 이용할 수 있다. 

* **from**: the node an edge is coming from
* **to**: the node an edge is going towards
* **weight**: A numeric value associated with each edge

[igraph](http://igraph.org/) package는 네트워크를 조작하고 분석하는 강력한 기능을 많이 가지고 있다. tidy object에서 igraph object를 생성하는 한 가지 방법은 `graph_from_data_frame()`를 이용하는 것이다. 이 함수는 "from", "to", edge attributes (이 경우에는 `n`)을 가지는 데이터 프레임을 만들어 준다.  

```{r bigram_graph, dependson = "bigram_counts"}
library(igraph)

# original counts
bigram_counts

# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

bigram_graph
```

igraph에는 plotting기능이 내장되어 있지만 pacakage가 수행하도록 설계된 기능이 아니기 때문에 다른 package에서 시각화 할 수 있는 방법이 개발되었다. 시각화에 쓸 수 있는 package로 `ggplot`package를 권장한다. ggplot2로 친숙한 그래픽 문법을 이용하여 시각화를 구현할 수 있기 때문이다. 

`ggraph()`를 이용하여 igraph object를 ggraph로 변환 할 수 있다. 그런 다음 ggplot2에서 레이어를 추가하는 것처럼 레이어를 추가한다. 예를 들어 기본 그래프의 경우 nodes, edges, text와 같이 세 겹의 레이어를 추가해야 한다.

```{r bigramgraph, dependson = "bigram_graph", fig.width = 9, fig.height = 7, fig.cap = "Common bigrams in Jane Austen's novels, showing those that occurred more than 20 times and where neither word was a stop word"}
library(ggraph)
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

그림 4.4에서 텍스트 구조의 일부분을 시각화 할 수 있다. 예를 들어 "miss", "lady", "sir", "colonel"과 같은 인사말이 노드의 공통 중심을 형성하고 종종 이름 뒤에 배치되는 것을 알 수 있다. 또한 일반적인 짧은 문구("half hour", "thousand pounds", or "short time/pause")를 형성하는 bigram이나 trigram이 있음을 알 수 있다.

더 나은 그래프로 다듬기 위해 몇 가지 작업으로 마무리 한다. (그림 4.5 참조)  

* link layer에 `edge_alpha` 속성을 주어 bigram의 빈도에 따른 투명도를 주고,  
* node에 연결될 때 화살표가 있도록 하는 `end_cap`을 설정한 `grid::arrow()`를 입력하고,  
* node를 보기 좋게 만들기 위해 node layer에 `color = "lightblue", size = 5`를 설정한다.  
* 마지막으로 배경을 없애기 위해 `theme_void()`를 적용한다.  

```{r bigramggraphausten2, dependson = "bigram_graph", fig.width = 9, fig.height = 7, fig.cap = "Common bigrams in Jane Austen's novels, with some polishing"}
set.seed(2020)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

네트워크를 이와 같이 표현하려면 ggraph로 변환하는 어려움이 있지만 네트워크 구조를 시각화 하는데 유용한 방법이다.  

이러한 텍스트 처리 방식은 **Markov chain**의 시각화라 하는데, Markov chain에서 각 단어의 선택은 이전 단어에만 의존한다. 이 경우 이 모델을 따르는 랜덤 값은 가장 높은 빈도를 보이는 단어 "dear", "sir", "william/walter/thomas/thomas's"를 순서대로 나타낸다. 시각화를 해석 가능하게 만들기 위해 높은 빈도의 단어끼리 연결되도록 표시했지만 텍스트에서 발생하는 모든 연결을 나타내는 거대한 그래프도 만들 수 있다.  


### Visualizing bigrams in other texts

텍스트 데이터 셋에서 bigram을 정리하고 시각화 하는데 많은 작업을 수행해봤다. 이제 다른 텍스트 데이터 셋에서 이를 쉽게 할 수 있도록 함수로 만드는 코드를 짜보겠다.  

`count_bigrams()`과 `visualize_bigrams()`을 쉽게 사용할 수 있도록 필요한 package를 다시 로드한다.   

```{r visualize_bigrams}
library(dplyr)
library(tidyr)
library(tidytext)
library(ggplot2)
library(igraph)
library(ggraph)

count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}
```

여기서 우리는 King James Version의 the Bible 같은 작품을 bigram을 토큰화 하여 시각화 해본다.  

```{r eval = FALSE}
# the King James version is book 10 on Project Gutenberg:
# library(gutenbergr)
# kjv <- gutenberg_download(10)
```

```{r kjv, echo = FALSE}
load("data/kjv.rda")
```

```{r kjvbigrams, dependson = c("kjv", "visualize_bigrams"), fig.width = 9, fig.height = 7, fig.cap = "Directed graph of common bigrams in the King James Bible, showing those that occurred more than 40 times"}
library(stringr)

kjv_bigrams <- kjv %>%
  count_bigrams()

# filter out rare combinations, as well as digits
kjv_bigrams %>%
  filter(n > 40,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
  visualize_bigrams()
```

그림 4.6에서 "thy"와 "thou" (이 단어들은 stopwords로 간주될 수도 있음에 주의!)에 초점을 맞춘 성경내 공통 언어를 보여준다.  `count_bigrams()`과 `visualize_bigrams()` 은 bigram을 시각화 하는 기능을 구현하는 함수들이다.  

## Counting and correlating pairs of words with the widyr package

n-gram으로 토큰화하는 것은 인접한 단어 쌍을 탐색하는 유용한 방법이다. 그러나 인접하지 않은 특정 문서나 특정 챕터 내에서 동시에 나타나는 단어를 찾기 원할 수도 있다.  

Tidy data는 변수를 비교하거나 행별로 그룹화하는데 유용한 구조지만 행을 비교하는 것은 어려울 수 있다. 예를 들어 두 단어가 동일한 문서 내에서 나타나는 횟수를 세거나 두 단어가 얼마나 상관관계가 있는지 확인하기 위해서는 데이터를 wide matrix형태로 먼저 변환해야 한다.  

```{r widyr, echo = FALSE, out.width = '100%', fig.cap = "The philosophy behind the widyr package, which can perform operations such as counting and correlating on pairs of values in a tidy dataset. The widyr package first 'casts' a tidy dataset into a wide matrix, performs an operation such as a correlation on it, then re-tidies the result."}
knitr::include_graphics("https://www.tidytextmining.com/images/tmwr_0407.png")
```

5장에서 tidy text를 wide matrix로 변환하는 몇 가지 방법을 알아보겠지만 이 경우에는 필요하지 않다. [widyr](https://github.com/dgrtwo/widyr) package는 데이터 확장, 작업 수행, 데이터를 다시 원래 형태로 돌리는 일련의 작업 패턴을 단순화하여 갯수 및 상관관계 계산과 같은 작업을 쉽게 만들어준다. (그림 4.7 참조) 여기서는 관찰 그룹 사이 (예를 들어 문서나 텍스트 섹션 사이)를 쌍으로 비교하는 함수 사용에 초점을 둘 것이다.  

### Counting and correlating among sections

2장에서 감정 분석을 위해 더 넓은 범위를 선택하여 분석을 수행한 것처럼 "오만과 편견"을 10줄 단위로 나누어 분석을 해 본다. 같은 섹션에서 어떤 단어가 나타나는지 확인해보자.  

```{r austen_section_words}
austen_section_words <- austen_books() %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word)

austen_section_words
```

`widyr`package에서 유용한 함수 중 하나는 `pairwise_count()`로 접두사 'pairwise_'는 'word'열의 변수의 각 쌍에 대해 하나의 행을 생성한다는 것을 의미한다. 이를 통해 동일 섹션에서 함께 나타나는 빈도 수가 높은 단어 쌍을 확인할 수 있다.  

```{r count_pairs_words, dependson = "austen_section_words"}
library(widyr)

# count words co-occuring within sections
word_pairs <- austen_section_words %>%
  pairwise_count(word, section, sort = TRUE)

word_pairs
```

입력값은 문서(10줄로 구성된 한 섹션)와 단어의 각 쌍에 대해 하나의 행으로 구성되어 있고, 출력값은 각 단어 쌍이 하나의 행에 들어가 있다. 이 또한 tidy format으로 새로운 질문에 대해 사용할 수 있는 전혀 다른 구조이다.  

예를 들어 섹션에서 가장 흔한 단어 쌍은 "Elizabeth"와 "Darcy" (두 주인공)이다.아래와 같은 코드를 사용하여 "darcy"와 가장 자주 함께 나오는 단어를 쉽게 찾을 수 있다.  

```{r}
word_pairs %>%
  filter(item1 == "darcy")
```

### Pairwise correlation {#pairwise-correlation}

"Elizabeth"나 "Darcy"와 같은 단어 쌍은 가장 일반적으로 동시에 나타나는 단어(co-occurring words)이지만 *가장 일반적인 개별 단어*이기도 하므로 의미를 부여하기 어렵다. 대신에 그 단어들이 얼마나 자주 함께 나타나는지를 확인할 수 있게 단어들이 개별적으로 쓰이는 경우 대비 함께 사용되는 경우의 **상관관계(correlation)**를 확인해본다.  

특히 [phi coefficient](https://en.wikipedia.org/wiki/Phi_coefficient)에 초점을 맞출 것인데 이는 바이너리 상관관계를 측정하는 계수이다. 파이 계수는 단어 X와 Y가 **함께 나타나거나 함께 나타나지 않을 가능성이 각각 나타날 가능성보다 얼마나 더 높은지**를 정량적으로 나타낸 것이다.   

아래 테이블을 보자:

|  | Has word Y | No word Y | Total |  |
|------------|---------------|---------------|--------------|---|
| Has word X | $n_{11}$ | $n_{10}$ | $n_{1\cdot}$ |  |
| No word X | $n_{01}$ | $n_{00}$ | $n_{0\cdot}$ |  |
| Total | $n_{\cdot 1}$ | $n_{\cdot 0}$ | n |  |

예를 들어 $n_{11}$은 단어 X와 단어 Y가 모두 쓰여진 문서의 수를 나타내고 $n_{00}$는 둘 다 쓰여지지 않은 문서의 수를 나타낸다. $n_{10}$과 $n_{01}$은 단어가 다른 단어 없이 홀로 쓰여진 경우를 각각 나타낸다. 이 표를 사용하여 파이 계수를 나타내면 아래와 같다:

$$\phi=\frac{n_{11}n_{00}-n_{10}n_{01}}{\sqrt{n_{1\cdot}n_{0\cdot}n_{\cdot0}n_{\cdot1}}}$$

바이너리 데이터에 적용하는 파이 계수는 피어슨 상관계수와 동일하다.  

`widyr`package의 `pairwise_cor()`를 사용하면 단어가 한 섹션에서 얼마나 자주 나타나는지에 따라 단어 사이의 파이계수를 찾을 수 있게 해준다. `pairwise_count()`와 유사하게 사용하면 된다.  

```{r word_cors}
# we need to filter for at least relatively common words first
word_cors <- austen_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

word_cors
```

`pairwise_cor()`의 결과는 데이터 탐색에 유용하다. 예를 들어 `filter()`를 사용하면 "pounds"와 같은 단어와 상관관계가 가장 높은 단어를 쉽게 찾을 수 있다.  

```{r dependson = "word_cors"}
word_cors %>%
  filter(item1 == "pounds")
```

이처럼 특정한 단어를 선택하여 가장 관련성이 높은 단어를 찾을 수 있게 해준다. (그림 4.8 참조)  

```{r wordcors, dependson = "word_cors", fig.height = 6, fig.width = 6, fig.cap = "Words from Pride and Prejudice that were most correlated with 'elizabeth', 'pounds', 'married', and 'pride'"}
word_cors %>%
  filter(item1 %in% c("elizabeth", "pounds", "married", "pride")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()
```

ggraph를 이용하여 bigram을 시각화 한 것처럼 `widry`package를 사용하여 찾은 단어 클러스터와 상관관계를 시각화 할 수도 있다. (그림 4.9 참조)  

```{r wordcorsnetwork, dependson = "word_cors", fig.height = 7, fig.width = 8, fig.cap = "Pairs of words in Pride and Prejudice that show at least a .15 correlation of appearing within the same 10-line section"}
set.seed(2016)

word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

bigram 분석과 달리 이 그래프에서의 관계는 방향성을 가지는 것이 아니라 대칭성을 가진다. (그래서 화살표를 사용하지 않은 것임). 또한 "colonel/fitzwilliam"과 같이 bigram에 많이 보이는 이름과 직위의 단어 쌍이 많이 보이지만 "walk"와 "park" 혹은 "dance"와 "ball"과 같은 서로 가깝게 자주 쓰이는 단어 쌍들도 볼 수 있다.

## Summary

이 장에서는 tidy text 접근 방식으로 개별 단어를 분석하는 것뿐만 아니라 단어 간의 관계와 연결을 탐색하는 방법을 알아보았다. 이러한 관계를 탐색하기 위해서는 n-gram을 사용할 수 있으며, 이를 통해 어떤 단어가 다른 단어 뒤에 나타나는 경향이 있는지 또는 서로 근접하여 나타나는 단어들의 상관관계를 확인할 수 있다. 이 챕터에서 동시 발생하는 단어나 상관관계가 있는 단어들의 관계를시각화 하기 위해 `ggraph`package를 사용했다. 이러한 네트워크 시각화는 단어들간의 관계를 탐색하기 위한 유용한 도구로 이후 챕터에서 다뤄질 case study에서 중요하게 사용된다.  