---
title: 'Analyzing word and document frequency: tf-idf'
author: "J.H AHN"
date: '2022 1 24 '
output: 
  html_document:
    toc: TRUE
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Analyzing word and document frequency: tf-idf {#tfidf}

텍스트 마이닝 및 자연어 처리의 핵심은 문서의 내용을 어떻게 수량화 하는가이다. 문서를 구성하는 단어가 얼마나 중요한지 측정하기 위한 한가지 척도로 문서가 얼마나 자주 사용되는지를 나타내는 빈도수 *term frequency* (tf)를 들 수 있다. 그러나 문서에 자주 나타나지만 중요하지 않은 단어가 있을 수 있다. 예를 들어 "the", "is", "of" 등과 같은 것이다. 이러한 단어를 stopwords에 포함시켜 분석 전에 제거하는 접근 방식을 취할 수도 있지만 이러한 단어 중 일부는 중요한 역할을 할 수도 있다. stopwords에 등록하여 제거하는 방법은 일반적으로 사용되는 단어의 빈도를 조정하는 정교한 접근법은 아니다. 

또 다른 접근 방식으로 용어의 역문서빈도*inverse document frequency* (idf)를 확인하는 방법이 있다. 이는 일반적으로 사용되는 단어에 대한 가중치를 낮게 주고 문서 전체에서 고르게 많이 사용되지 않는 단어에 대해서는 가중치를 높게 부여하는 방식이다. 이것을 용어 빈도수와 결합하여 *tf-idf* (두 값의 곱으로 표현된 값 )을 계산할 수 있으며, 용어의 빈도는 사용 빈도에 따라 조정된다.  

이 방법은 'rule-of-thumb'으로 경험적인 법칙에 기반을 두고 있다. 텍스트 마이닝이나 검색 엔진 등에서 유용하다는 것이 입증되 었지만 정밀하고 확고한 이론적인 토대가 정립되지 않은 상태이다. 주어진 용어에 대한 idf는 아래와 같이 정의된다.  


$$idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}$$

1장에서 설명된 대로 tidy data 원칙을 사용하여 tf-idf분석을 하면 일관되고 효과적으로 문서에서 단어 들이 얼마나 다양하고 중요한지를 정량화 할 수 있다.  

## Term frequency in Jane Austen's novels

제인 오스틴의 소설을 이용하여 용어빈도를 조사한 후 tf-idf를 살펴본다. `group_by()`와 `join()`과 같은 dplyr를 사용하여 필요한 내용만 담기도록 정리할 수 있다. 제인 오스틴 소설에서 가장 일반적으로 사용되는 단어는 아래와 같다.  

```{r book_words}
library(dplyr)
library(janeaustenr)
library(tidytext)

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE)

total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words
```

`book_words` data frame에는 각 단어가 사용된 빈도수가 정리되어있다. 'n'열에는 해당 책에서 해당 단어가 사용된 횟수가 들어 있고, 'total'열에는 해당 책의 총 단어의 수가 들어있다. 여기서 가장 많은 횟수로 사용된 단어는 "the", "and", "to"임을 확인 할 수 있다. 아래 그림에서는 총 단어 수 대비 해당 단어가 사용된 횟수의 비율인 `n/total`의 분포를 시각화 하였다. 총 단어 수 대비 해당 단어가 사용된 횟수가 *term frequency* (tf)이다. 

```{r plottf, dependson = "book_words", fig.height=6, fig.width=6, fig.cap="Term frequency distribution in Jane Austen's novels"}
library(ggplot2)

ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")
```

그래프를 보면 X축의 오른쪽으로 long-tail이 형성되어 있다. 그래프는 자주 쓰이지 않은 많은 단어와 자주 쓰이는 적은 수의 단어가 모든 소설에서 유사하게 나타남을 알 수 있게 해준다.  

## Zipf's law

그림 3.1에 나타난 것과 같은 분포는 언어에서 일반적인 형태이다. 사실 이러한 유형의 long-tail 분포는 주어진 자연어 모음(예, 책, 웹사이트 등)에서 너무 일반적이어서 단어가 사용되는 빈도와 그 순위는 연구의 주제였다. 이 관계의 고전적인 버전은 20세기 미국 언어학자인 George Zipf의 이름을 따서 Zipf의 법칙이라고 한다. 

Zipf's law에 따르면 단어가 나타나는 빈도는 순위에 반비례한다. (당연한데 이게 왜 법칙이지??)

**Zipf's law 추가 설명**  

지프의 법칙에 따르면 어떠한 자연어 말뭉치 표현에 나타나는 단어들을 그 사용 빈도가 높은 순서대로 나열하였을 때, 모든 단어의 사용 빈도는 해당 단어의 순위에 반비례한다. 따라서 가장 사용 빈도가 높은 단어는 두 번째 단어보다 빈도가 약 두 배 높으며, 세 번째 단어보다는 빈도가 세 배 높다. 예를 들어, 브라운 대학교 현대 미국 영어 표준 말뭉치의 경우, 가장 사용 빈도가 높은 단어는 영어 정관사 “the”이며 전체 문서에서 7%의 빈도(약 백만 개 남짓의 전체 사용 단어 중 69,971회)를 차지한다. 두 번째로 사용 빈도가 높은 단어는 “of”로 약 3.5% 남짓(36,411회)한 빈도를 차지하며, 세 번째로 사용 빈도가 높은 단어는 “and”(28,852회)로, 지프의 법칙에 정확히 들어 맞는다. 약 135개 항목의 어휘만으로 브라운 대학 말뭉치의 절반을 나타낼 수 있다.

지프의 법칙은 도시의 인구 순위나 기업의 크기, 소득 순위 등과 같은 언어학과 관련이 없는 다른 여러가지 순위에서도 동일하게 발견된다. 도시의 인구 순위 분포에서 발견되는 현상은 1913년 독일의 펠릭스 아워바흐에 의해 처음 발견되었다. 경험적으로, 특정 데이터의 집합에 지프의 법칙이 적용되는지는 데이터의 순위 R, 해당 데이터의 값 n, 그리고 상수값 a, b로 이루어지는 로그 회귀 R = a - b log n을 적용함으로써 확인 가능하다. 지프의 법칙은 b = 1일 때 적용된다. 이 회귀함수가 도시의 크기에 적용될 경우, b = 1.07일 때 더 정확히 맞아 떨어진다. 지프의 법칙은 도시 크기 분포의 상위 항목들에 적용되며, 전체 도시 크기 분포는 로그정규분포이며 지브라의 법칙을 따른다. 지프의 법칙과 지브라의 법칙은 서로 일치하는데, 이것은 로그정규분포의 꼬리가 일반적으로 파레토(지프) 분포의 꼬리와 구분되지 않기 때문이다.

용어 빈도를 나타내는데 사용한 데이터 프레임이 있으므로 몇 줄의 dplyr함수를 이용하여 제인 오스틴 소설에 대한 Zipf의 법칙을 확인해 볼 수 있다. 

```{r freq_by_rank, dependson = book_words}
freq_by_rank <- book_words %>% 
  group_by(book) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

freq_by_rank
```

여기에서 `rank`열은 빈도 테이블 내에서 각 단어의 순위값이 들어가 있다. 테이블은 이미 'n'열에 빈도수가 정리되어 있으므로 `row_number()`를 사용하여 빈도 순위를 찾을 수 있다. 그런 다음 이전과 같은 방식으로 용어 빈도(term frequency)를 찾을 수 있다. Zipf의 법칙은 X축은 순위, Y축은 빈도로 하여 X,Y축 모두 로그처리하여 시각화한다. 이런 식으로 그래프를 그리면 반비례 관계는 일정하고 음의 기울기를 갖게 된다. 

```{r zipf, dependson = "freq_by_rank", fig.width=5, fig.height=4.5, fig.cap="Zipf's law for Jane Austen's novels"}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

그림 3.2는 log-log 좌표 그래프이다. 제인 오스틴의 소설 6평은 모두 서로 비슷하며 순위와 빈도의 관계가 음의 기울기를 가지고 있음을 알 수 있다. 그러나 음의 기울기가 일정하지는 않고 세 부분으로 나눠진 멱법칙(power law)로 나타남을 볼 수 있다. 순위 범위 중간에 부분의 거듭제곱의 지수가 무엇인지 확인해보자.  

**멱법칙(power law)**  

한 수가 다른 수의 거듭제곱으로 표현되는 두 수의 함수적 관계를 의미한다. 예를 들어, 특정 인구수를 가지는 도시들의 숫자는 인구수의 거듭제곱에 반비례하여 나타난다. 경험적인 멱법칙 분포는 근사적으로만, 또는 제한된 범위에서만 적용된다.   
모든 값이 멱법칙을 따르는 분포는 드물며, 대부분 분포의 꼬리 부분에서만 멱법칙이 적용된다. 다양한 복합 매질에서 발생하는 음파 감쇠의 경우에는 넓은 주파수 대역에 걸쳐 멱법칙을 따른다. 인구 순위를 나타내는 데 사용되는 멱법칙 그래프를 보면 오른쪽은 롱테일의 형태를 띠며, 왼쪽은 전체에서 절대적인 비율을 차지하는 소수의 도시들을 나타난다. 이러한 형태는 파레토 법칙으로도 알려져 있다.  

```{r lower_rank, dependson = "freq_by_rank"}
rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
```

고전적인 버전의 Zipf's law는 아래와 같다.  

$$\text{frequency} \propto \frac{1}{\text{rank}}$$
실제로 거듭제곱의 지수가 -1에 가까운 기울기를 가지는 것으로 확인되었다. 이 피팅된 멱법칙(거듭제곱 법칙)을 그래프로 그려본다.  

```{r zipffit, dependson = "freq_by_rank", fig.width=5, fig.height=4.5, fig.cap="Fitting an exponent for Zipf's law with Jane Austen's novels"}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_abline(intercept = -0.62, slope = -1.1, 
              color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

그래프를 보면 제인 오스틴 소설의 말뭉치에 대해 Zipf's law의 고전적인 버전에 가까운 결과가 나옴을 확인할 수 있다. 높은 순위에서 볼 수 있는 편차는 많은 종류의 언어에서도 유사하게 나타난다. 언어의 말뭉치에는 종종 멱법칙으로 예측되는 것보다 적은 수의 희귀 단어가 포함된다. 낮은 순위의 편차는 더 이례적(more unusual)이다. 제인 오스틴은 멱법칙에 의해 예측되는 것보다 더 적게 낮은 순위의 단어를 사용한다는 것을 알 수 있다. 이러한 분석은 저자를 비교하거나 다른 텍스트와 비교하는데 사용할 수도 있다.   

## The `bind_tf_idf()` function

tf-idf의 개념은 각 문서의 말뭉치 혹은 모음에서 일반적으로 사용되는 단어에 대한 가중치를 낮게 부여하고 많이 사용되지 않지만 특정 문서나 말뭉치에 높은 빈도로 나타나는 단어에 대한 가중치를 높게 부여하여 각 문서의 내용에 대해 중요한 단어를 찾는 것이다. 이 경우 제인 오스틴의 소설 전체 tf-idf를 계산하면 텍스트에서 중요하지만 지나치게 일반적이지 않은 단어를 찾을 수 있다.  

`tidytext`package의 `bind_tf_idf()`는 각 문서의 토큰이 하나의 행에 들어있는 tidy format의 텍스트 데이터 셋을 입력값으로 사용한다. 한 열에는 토큰(단어)이 있어야 하고, 다른 한 열에는 문서(이 경우에는 책), 또 다른 한 열에는 갯수 즉 각 문서에서 나타난 각 단어의 빈도(n)가 들어 있어야 한다. 이전 섹션에서 유사한 분석을을위해 각 책에 대해 total값은 계산했지만 `bind_tf_idf()`를 사용하고자 할 때는 필요하지 않고, 테이블에는 각 문서의 모든 단어만 포함되면 된다.  

```{r tf_idf, dependson = "book_words"}
book_tf_idf <- book_words %>%
  bind_tf_idf(word, book, n)

book_tf_idf
```

idf와 tf-idf는 이러한 매우 일반적인 단어들에 대해서는 0(zero)이다. 이것들은 모두 제인 오스틴의 소설 6편에 나오는 단어 들이므로 idf항(1의 자연로그 값)은 0이 된다. 역문서빈도(tf-idf)는 많은 문서들 모음에서 발생하는 단어에 대해 매우 낮은 값을 가진다. (거의 0에 가까움). 이런 식으로 일반적으로 문서 전반에 나타나는 단어에 대한 가중치를 낮게 부여한다. 역문서빈도(tf-idf)는 특정 문서 모음에서 높은 빈도로 나타나는 단어들에 대해서는 높은 값의 가중치를 부여한다.  

제인 오스틴의 작품에서 높은 tf-idf값을 가지는 단어들을 살펴보자.  

```{r desc_idf, dependson = "tf_idf"}
book_tf_idf %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

여기에서 소설에 등장했던 고유 명사들을 볼 수 있다. 이 단어들은 모든 제인 오스틴 소설에 쓰여지지 않았고 각 소설에서 중요하고 특징적인 단어로 쓰여진 것들이다.  

idf의 몇몇 값들은 다른 단어들과 같은 값을 가지는데 그것은 이 말뭉치 내에 6개 소설이 있고 $\ln(6/1)$, $\ln(6/2)$ 등의 숫자형 값을 가지기 때문이다.   


이러한 높은 tf-idf값을 가지는 단어들을 시각화 해보자.  

```{r plotseparate, dependson = "plot_austen", fig.height=8, fig.width=6, fig.cap="Highest tf-idf words in each of Jane Austen's Novels"}
library(forcats)

book_tf_idf %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

여전히 모든 고유명사는 그림 3.4에 있음을 알 수 있다. tf-idf로 측정한 이 단어는 각 소설에서 가장 중요한 단어들이다. tf-idf를 측정한 것은 제인 오스틴이 그녀의 소설 6편에서 비슷한 언어를 사용했고, 그녀의 작품 모음에서 한 소설을 다른 소설과 구별하는 것은 고유명사, 사람과 장소라는 것을 보여준다. 문서 모음 내에서 하나의 문서를 구별할 수 있는 중요한 단어를 식별해 내는 것이 tf-idf의 핵심이다.  

## A corpus of physics texts

다른 문서 모음에서는 어떤 단어가 중요한지 알아보자. 이번에는 고전 물리학 서적을 다운로드 받아 tf-idf로 각 책에서 어떤 단어가 중요하게 다뤄지는지 알아볼 것이다.    

아래 서적을 다운로드 받는다.  
* [*Discourse on Floating Bodies* by Galileo Galilei](http://www.gutenberg.org/ebooks/37729)  
* [*Treatise on Light* by Christiaan Huygens](http://www.gutenberg.org/ebooks/14725)  
* [*Experiments with Alternate Currents of High Potential and High Frequency* by Nikola Tesla](http://www.gutenberg.org/ebooks/13476)  
* [*Relativity: The Special and General Theory* by Albert Einstein](http://www.gutenberg.org/ebooks/30155)  

이 서적들은 다양한 분야에 걸쳐있고 모두 물리학의 고전이며 300년간에 걸쳐 쓰여진 책들이다. 일부는 다른 언어로 쓰여있다가 나중에 영어로 번역된 서적도 있어 완벽하게 균질하지는 않지만 분석에는 아무런 문제가 없다.  

```{r eval = FALSE}
library(gutenbergr)
physics <- gutenberg_download(c(37729, 14725, 13476, 30155), 
                              meta_fields = "author")
```

```{r physics, echo = FALSE}
load("data/physics.rda")
```

데이터를 받았으니 `unnest_tokens()`과 `count()`를 사용하여 각 텍스트에서 각 단어들이 얼마나 많이 쓰여졌는지 알아보자.  

```{r physics_words, dependson = "physics"}
physics_words <- physics %>%
  unnest_tokens(word, text) %>%
  count(author, word, sort = TRUE)

physics_words
```

여기에서 갯수를 볼 수 있는데 중요한 것은 각 텍스트의 길이가 다르다는 것이다. 일단 tf-idf를 계산한 다음 그림 3.5에서 높은 tf-idf 단어들을 시각화 본다.  

```{r physicsseparate, dependson = "plot_physics", fig.height=6, fig.width=6, fig.cap="Highest tf-idf words in each physics texts"}
plot_physics <- physics_words %>%
  bind_tf_idf(word, author, n) %>%
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan", 
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))

plot_physics %>% 
  group_by(author) %>% 
  slice_max(tf_idf, n = 15) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~author, ncol = 2, scales = "free")
```

매우 흥미로운 결과가 나왔는데 아인슈타인의 책에서 "_k_"가 높은 tf-idf에 포함되어 있는 것이다.

```{r dependson = "physics"}
library(stringr)

physics %>% 
  filter(str_detect(text, "_k_")) %>% 
  select(text)
```

아인슈타인 텍스트의 높은 tf-idf 단어에는 "co" 및 "ordinate" 항목도 있는데 이것들도 정리해야 한다. `unnest_tokens()` 함수는  기본적으로 구두점 주변을 -(hypens)처럼 분리한다. 그래서 "co"와 "ordinate"의 tf-idf가 거의 같게 나타난다. 

"AB", "RC" 등은 하위헌스(Huygens' Principle, 영어식 발음은 호이겐스)에 대한 rays, circles, angles을 의미한다.

```{r dependson = "physics"}
physics %>% 
  filter(str_detect(text, "RC")) %>% 
  select(text)
```

덜 중요한 단어를 제거하여 중요한 단어들로 구성한다. stopwords의 사용자 지정목록을 만들고 `anti_join()`을 사용하여 제거한다. 이는 많은 상황에서 유용하게 사용할 수 있는 방법이다. tidy data frame에서 단어를 제거해야 하기 때문에 몇 단계 이전에서 부터 시작해야 한다.  

```{r mystopwords, dependson = "plot_physics", fig.height=6, fig.width=6, fig.cap="Highest tf-idf words in classic physics texts"}
mystopwords <- tibble(word = c("eq", "co", "rc", "ac", "ak", "bn", 
                                   "fig", "file", "cg", "cb", "cm",
                               "ab", "_k", "_k_", "_x"))

physics_words <- anti_join(physics_words, mystopwords, 
                           by = "word")

plot_physics <- physics_words %>%
  bind_tf_idf(word, author, n) %>%
  mutate(word = str_remove_all(word, "_")) %>%
  group_by(author) %>% 
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  mutate(word = fct_reorder(word, tf_idf)) %>%
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan",
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))

ggplot(plot_physics, aes(tf_idf, word, fill = author)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~author, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```


이 장에서 본 제인 오스틴의 소설이나 물리학 서적의 경우 높은 tf-idf를 가진 단어들이 많이 중복되지는 않았다. 만약 tf-idf가 전 범주에 걸쳐 중복되는 경우에는 `reorder_within()`과 `scale_*_reordered()`를 사용하여 시각화하여야 한다. 


## Summary 

단어 빈도(term frequency)와 역문서빈도(inverse document frequency)를 사용하면 해당 문서의 종류에 관계없이 문서 모음 내에서 한 문서에 대한 특징적인 단어를 찾을 수 있다. 단어 빈도(term frequency)를 사용하면 자연어 모음에서 언어가 사용되는 방식에 대해 이핼 수 있으며 `count()`나 `rank()`와 같은 dplyr함수를 이용하여 쉽게 찾을 수 있다. `tidytext`package는 문서 모음 또는 문서 모음 내 한 문서에서 단어가 얼마나 중요한지 확인 할 수 있도록 tidy data 원칙과 일치하는 tf-idf값을 찾을 수 있게 해준다.  
