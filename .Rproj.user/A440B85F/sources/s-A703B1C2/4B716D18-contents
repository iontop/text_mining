---
title: "The tidy text format"
author: "J.H AHN"
date: '2022 1 21 '
output: 
  html_document:
    toc: TRUE
    number_sections: TRUE
---

# The tidy text format {#tidytext}

Tidy 데이터 원칙을 사용하는 것은 데이터를 보다 쉽고 효과적으로 처리하는 강력한 방법이며 텍스트를 다룰 때도 마찬가지임.  

* 각 변수 = 열(column)
* 각 관측값 = 행(row)
* 관측 단위의 각 유형 = 표(table)

위 규칙에 따라 Tidy text 형식을 **행당 하나의 토큰이 있는 테이블**로 정의함. 토큰(token)은 분석에 사용하려는 단어와 같은 의미의 텍스트 단위이며 토큰화(tokenization)는 텍스트를 토큰으로 분해하는 프로세스를 말한다.  한 행에 하나의 토큰이 있는 구조는 텍스트가 문서나 문장에 저장되는 방식과 대조된다. Tidy text mining을 위해 각 행에 저장되는 토큰은 대부분 단일 단어이지만 n-gram, 문장 또는 단락이 될 수도 있다.  

Tidy package에는 text mining에 많이 사용되는 `tm` packag나 `quanteda` pacakge로 가져온 것을 `tidy()` object로 만들 수 있는 기능을 가지고 있다. 이를통해 dplyr 및 tidy를 사용하여 가져오기, 필터링 등의 작업을 수행한 후 Machine learning을 위한  document-term matrix로 변환할 수 있다. 그 다음 ggplot2를 사용하여 시각화 할 수도 있다.  

## Contrasting tidy text with other data structures

위에서 언급했듯이 Tidy **text 포맷을 행당 하나의 토큰**이 있는 테이블로 정의한다. 이러한 방식으로 텍스트 데이터를 구조화 한다는 것은 tidy data 원칙을 준수하고 동일한 툴로 조정할 수 있다는 것을 의미한다.  

* **String**: 텍스트는 문자열 = 문자 벡터로 저장될 수 있다.  
* **Corpus**: 말뭉치(Corpus)유형의 Object는 추가 메타데이터 및 세부 정보가 주석으로 달린다.
* **Document-term matrix**: 각 문서에 대해 하나의 행과 각 용어에 대해 하나의 열이 있는 Corpus를 나타내는 희소 행렬이다. 행렬의 값은 일반적으로 단어 수 또는 tf-idf이다.  

5장에서 Corpus와 Document-term matrix에서 다룰 예정이다.  

## The `unnest_tokens` function

```{r text}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")

text
```

이런 형태는 분석하는 일반적인 문자 벡터 형태이다. 이것을 tidy text dataset으로 변환하기 위해 data frame으로 만들어 준다.  

```{r text_df, dependson = "text"}
library(dplyr)
text_df <- tibble(line = 1:4, text = text)

text_df
```

tidytext의 `unnest_tokens()` function을 사용하여 토크나이저 시킨다.  

```{r dependson = "text_df", R.options = list(dplyr.print_max = 10)}
library(tidytext)

text_df %>%
  unnest_tokens(word, text)
```

`unnest_tokens()`에 사용되는 argument는 두 가지가 있다. 분해한 텍스트를 출력해 줄 열이름 (이 경우 'word')과 텍스트를 가져 올 입력 열(이 경우 'text')을 넣어줘야 한다.  

`unnest_tokens()`를 사용하여 각 행에 하나의 토큰(단어)가 있도록 각 행을 나눴다.  

* 각 단어의 행 번호와 같은 다른 열은 동일하게 유지된다.  
* 구두점은 제거된다.  
* Default로 소문자로 변환하는데 소문자로 변환을 하지 않으려면 'to_lower=FALSE'로 입력한다.  


## Tidying the works of Jane Austen {#tidyausten}

제인 오스틴 소설 6권의 텍스트를 tidy형식으로 변환해 본다.  
`mutate()`를 사용해서 chapter 열에 chapter 번호를 넣는다.  
chapter 번호를 넣기 위해 정규표현식`regex()`을 사용한다. regex("^chapter)에서 [^]는 해당 문자를 제외한 모든 것을 의미한다. 즉  c,h,a,p,t,e,r을 제외한 나머지에 대해서 ignore_case는 대소문자 상관없이 찾을 것이냐는 의미로 TRUE값을 넣으면 대소문자 관계없이 찾겠다는 의미이다.  

```{r original_books}
library(janeaustenr)
library(dplyr)
library(stringr)

original_books <- 
  austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                                     regex("^chapter [\\divxlc]",
                                           ignore_case = TRUE)))) %>%
  ungroup()

original_books
```

tidy dataset으로 만들었고 이제 한 행에 하나의 토큰만 있도록 `unnest_tokens()`를 사용하여 변환한다.  

```{r tidy_books_raw, dependson = "original_books"}
library(tidytext)
tidy_books <- original_books %>%
  unnest_tokens(word, text)

tidy_books
```

`unnest_tokens()`는 [tokenizers](https://github.com/ropensci/tokenizers) package를 사용하여 원본 데이터 프레임의 텍스트 라인을 토큰으로 분리한다. 옵션에 따라 단어 뿐만 아니라 n-gram, 문장, 줄, 단락 또는 정규식 패턴의 한 분리도 가능하다.  

이제 데이터를 한 행에 한 단어가 들어가는 형식으로 변경하였기 때문에 dplyr과 같은 tidy tool을 사용하여 데이터를 조작할 수 있다. `anti_join()`을 이용하여 stopwords를 제거한다.  

```{r tidy_books, dependson = "tidy_books_raw"}
data(stop_words)

tidy_books <- tidy_books %>%
  anti_join(stop_words)
```

Tidytext package의 `stop_words` dataset에는 세 종류 lexicons(사전, 어휘목록)의 stop words가 포함되어 있다. 모두 사용할 수도 있고, 필요하다면 `filter()`를 써서 한 종류만 쓸 수도 있다.  

또한 dplyr의 `count()`를 사용하여 책 전체에서 가장 일반적인 단어 수를 확인할 수 있다.   

```{r dependson = "tidy_books"}
tidy_books %>%
  count(word, sort = TRUE) 
```

tidy tool을 사용하였기 때문에 단어는 tidy 형식의 데이터 프레임에 저장된다. 이 경우 ggplot2를 이용하여 바로 시각화를 할 수 있다.  

```{r plotcount, dependson = "tidy_books", fig.cap="The most common words in Jane Austen's novels"}
library(ggplot2)

tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

`austen_books()`은 텍스트만 잘 정리해 둔 상태인데 일반적인 경우에는 저작권 관련 내용같은 필요 없는 내용을 제거해야 할 수도 있다. 이런 내용은 이후 Chap 9.1.1에서 다루도록 한다.   

## The gutenbergr package

gutenbergr 패키지는 책 다운로들르 위한 도구와 관심있는 작품을 찾는데 사용할 수 있는 프로젝트 구텐베르크 메타데이터 전체가 포함되어 있다. 예제에서는 ID를 이용하여 작품을 다운로드 하는 `gutenberg_download()`를 사용하지만 다른 함수를 사용하여 메타 데이터를 찾고 Gutenberg ID와 해당되는 제목, 저자, 언어등을 찾을 수도 있다.  

## Word frequencies

텍스트 마이닝의 일반적인 작업은 위의 예제와 같이 단어 빈도를 확인하고 다른 텍스트와 단어 빈도를 비교하는 것이다. 제인 오스틴 작품을 가지고 있으므로 비교를 위해 H.G 웰스의 SF 소설과 판타지 소설을 가져오자.  

* [*The Time Machine*](https://www.gutenberg.org/ebooks/35)  
* [*The War of the Worlds*](https://www.gutenberg.org/ebooks/36)  
* [*The Invisible Man*](https://www.gutenberg.org/ebooks/5230)  
* [*The Island of Doctor Moreau*](https://www.gutenberg.org/ebooks/159)  

`gutenberg_download()`에 각 소설의 ID를 넣어 가져온다. 
'22년 1월 현재 `gutenberg_download()`로 다운받는 것이 안되므로 *.rda 파일을 로드하여 사용한다.  

```{r eval=FALSE}
library(gutenbergr)

# hgwells <- gutenberg_download(c(35, 36, 5230, 159))
```

```{r hgwells, echo = FALSE}
load("data/hgwells.rda")
```

```{r tidy_hgwells, dependson = "hgwells"}
tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
```

H.G wells 소설에서 빈도가 높은 단어를 확인한다.  

```{r dependson = "tidy_hgwells"}
tidy_hgwells %>%
  count(word, sort = TRUE)
```

브론테 자매의 작품을 가져온다.  
* [*Jane Eyre*](https://www.gutenberg.org/ebooks/1260)  
* [*Wuthering Heights*](https://www.gutenberg.org/ebooks/768)  
* [*The Tenant of Wildfell Hall*](https://www.gutenberg.org/ebooks/969)  
* [*Villette*](https://www.gutenberg.org/ebooks/9182)  
* [*Agnes Grey*](https://www.gutenberg.org/ebooks/767). 

`gutenberg_download()`로 다운받는 것이 안되므로 *.rda 파일을 로드하여 사용한다.  

```{r eval = FALSE}
# bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))

```

```{r echo = FALSE}
load("data/bronte.rda")
```

```{r tidy_bronte, dependson = "bronte"}
tidy_bronte <- bronte %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
```

Brontë sisters 소설에서 가장 빈도가 높은 단어를 확인한다.  

```{r dependson = "tidy_bronte"}
tidy_bronte %>%
  count(word, sort = TRUE)
```

두 작가 모두 "time", "eyes", and "hand" 가 상위 10위에 들어있다.  

이제 `pivot_longer()`와 `pivot_wider()` 적절히 사용하여 세 작가의 작품을 시각화 시키기 편한 형태로 만든다.  

```{r frequency, dependson = c("tidy_bronte", "tidy_hgwells", "tidy_books")}
library(tidyr)

frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = author, values_from = proportion) %>%
  pivot_longer(`Brontë Sisters`:`H.G. Wells`,
               names_to = "author", values_to = "proportion")

frequency
```

`mutate()`에서 `str_extract()`를 사용하는데 프로젝트 구텐베르그는 UTF-8로 인코딩되어 강조하는 단어를 나타내기 위해 주변에 밑줄이 있는 단어가 있기 때문이다. 토크나이저하면서 이를 단어로 처리했지만 "\_any\_"를 "any"와 별개의 단어로 계산하는 것을 피하기 위해서 `str_extract()`를 사용한다. '[a-z]+'는 정규표현식으로 [a-z]는 알파벳 소문자 중 1개를 의미하고 +(더하기)는 앞 문자가 1개 이상인 경우르ㄹ의미한다. 즉 앞에 알파벳 소문자가 1개라도 있으면 글자로 보고 반환하라는 의미이다.      


```{r plotcompare, dependson = "frequency", fig.width=10, fig.height=5, fig.cap="Comparing the word frequencies of Jane Austen, the Brontë sisters, and H.G. Wells"}
library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, 
                      color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```

그래프에서 abline에 가까운 단어는 두 작가의 작품에서 유사한 빈도를 가진다. 예를 들어 제인 오스틴 브론테 자매의 텍스트를 보면 "miss", "time", "day"가 비슷하게 높은 빈도로 나오고, 제인 오스틴과 H.G 웰스는 "time", "day", "brother"가 비슷하게 높은 빈도로 나타난다. 전반적으로 제인 오스틴과 브론테 자매가 상대적으로 더 비슷한 단어를 사용하고 있음을 알 수 있다.  

이런 단어 빈도의 집합을 상관테스트를 이용하여 얼마나 유사한지 정량화할 수 있다.  

```{r cor_test, dependson = "frequency"}
cor.test(data = frequency[frequency$author == "Brontë Sisters",],
         ~ proportion + `Jane Austen`)
cor.test(data = frequency[frequency$author == "H.G. Wells",], 
         ~ proportion + `Jane Austen`)
```

그래프에서 확인 된 것처럼 제인 오스틴과 H.G 웰스(0.424162)보다 제인 오스틴과 브론테 자매(0.7609907)의 상관성이 더 높은 것으로 나타난다.  

## Summary

이 장에서 텍스트를 tidy 원칙을 이용하여 자연어를 어떻게 처리할 수 있는지를 배웠다. 텍스트가 하나의 행에 하나의 토큰이 있는 형식이라면 불용어 제거, 단어 빈도 계산과 같은 작업은 tidy tool을 이용하여 쉽게 할 수 있다. 하나의 행에 하나의 토큰이 있는 프레임 워크는 단일 단어에서 n-gram 및 기타 의미있는 텍스트 단위로 확장할 수 있을 뿐 아니라 이 책에서 고려할 다른 많은 분석 으로 확장할 수 있다.  
