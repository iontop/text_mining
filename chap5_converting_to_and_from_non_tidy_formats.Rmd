---
title: "Converting to and from non-tidy formats"
author: "J.H AHN"
date: '2022 1 26 '
output: 
  html_document:
    toc: TRUE
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Converting to and from non-tidy formats {#dtm}

이전 챕터에서 `unnest_tokens()`을 사용하여 한 행에 하나의 토큰이 들어있는 tidy text format으로 정렬된 텍스트를 분석했다.  dplyr, tidyr, ggplot2와 같은 tidy tools을 사용해서 쉽게 데이터를 탐색하고 시각화 할 수 있었다. tidy tools을 사용하여 많은 정보를 가지고 있는 텍스트를 분석할 수 있다는 것을 확인했다.  

그러나 `tidytext`package를 제외한 대부분의 자연어 처리를 위한 R tools들과 tidy tools은 호환되지 않는다. [CRAN Task View for Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html) 목록을 보면 많은 수의 패키지가 있는데 tidy 구조와 다른 형태의 입력과 출력값을 가진다. 이러한 패키지들은 텍스트 마이닝에 매우 유용하기 때문에 기존의 텍스트 데이터 셋들은 이 패키지들의 포맷을 따르도록 되어있는 경우가 많다.  

컴퓨터 과학자 Hal Abelson은 "개별 작업이 아무리 복잡하고 정교하더라도 시스템의 성능에 가장 직접적으로 영향을 미치는 것은 개별 작업을 연결하는 접착제의 품질"이라고 말했다. 이처럼 tidy text format을 다른 중요한 패키지나 데이터 구조와 연결하여 기존의 텍스트 마이닝 패키지를 사용할 수 있도록 하는 "접착제"에 대해서 알아본다.  

```{r tidyflowchartch5, echo = FALSE, out.width = '100%', fig.cap = "A flowchart of a typical text analysis that combines tidytext with other tools and data formats, particularly the tm or quanteda packages. This chapter shows how to convert back and forth between document-term matrices and tidy data frames, as well as converting from a Corpus object to a text data frame."}
knitr::include_graphics("https://www.tidytextmining.com/images/tmwr_0501.png")
```

그림 5.1은 tidy data와 non-tidy data를 전환하며 분석을 어떻게 수행하는지를 보여준다. 이번 챕터에서는 tidying document-term matrices와 tidy data fram을 희소행렬(sparse matrix)로 변환하는 내용에 초점을 둘 것이다. 또한 raw text와 문서의 메타데이터가 조합된 tidy Corpus object를 data frame으로 변환하고 이를 이용해 금융관련 기사를 수집, 분석하는 case study로 어떻게 이어지는지도 보여줄 것이다.   

## Tidying a document-term matrix {#tidy-dtm}

텍스트 마이닝 패키지 중 가장 일반적인 구조는 DTM[document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix)이라 불리는 것이다. 이 매트릭스는 다음과 같은 특징을 가지고 있다:

* 각 행은 하나의 문서(책, 기사)를 나타낸다.  
* 각 열은 하나의 단어(term)를 나타낸다.  
* (일반적으로) 각 값은 문서 내의 단어(term)가 얼마나 쓰여졌는지 빈도 수를 나타낸다.  

대부분의 문서나 단어는 쌍으로 나타나지 않기 때문에 (값이 0이기 때문에) DTM은 일반적으로 희소 행렬로 구현된다. 이러한 object는 행렬로 처리되지만 (특정 행과 열에 접근하는 방식으로 처리) 더 효율적인 형식으로 저장된다. 이 챕터에서는 이러한 행렬을 구현하는 방법에 대해 다룰 것이다.  

DTM object는 대부분의 텍스트 마이닝 패키지에 대한 입력으로 tidy tools을 바로 사용할 수 없다. 그러므로 `tidytext`package에서 제공하는 두 형식 사이를 변환할 수 있는 기능을 사용해야 한다.    

* `tidy()`는 document-term matrix를 tidy data frame으로 변환해준다. 이 함수는 많은 통계모델 및 object에 대해 유사한 형태로 정리해주는 기능을 제공하는 `broom`package에 포함되어 있다.  
* `cast()`는 tidy one-term-per-row data frame을 행렬(matrix)로 변환해준다. tidytext는 세 가지 형태의 cast함수를 제공하는데 예를 들어 `cast_sparse()`는 `Matrix` package를 이용해 희소 행렬(sparse matrix)로 변환시켜주고, `cast_dtm()`은 `tm`package를 document-term matrix형태로 변환시켜 준다. `cast_dfm()`은 `quanteda`package를 이용하여 dfm object로 변환해준다. 

그림 5.1에서 본 것처럼 DTM은 문서와 단어의 조합에 대한 여러 통계값이나 빈도 수를 포함된 tidy data frame와 유사하다.  

### Tidying DocumentTermMatrix objects

아마 R에서 가장 널리 사용되는 DTM 구현은 `tm`package의 `DocumentTermMatrix` class 일 것이다. 많은 수의 텍스트 마이닝 셋이 이 형식으로 제공된다. 예를 들어 `topicmodels`package에 들어있는 Associated Press 신문기사를 한 번 보자.  

```{r AssociatedPress}
library(tm)

data("AssociatedPress", package = "topicmodels")
AssociatedPress
```

이 데이터 셋에는 `r nrow(AssociatedPress)` documents(각각의 AP신문기사)와 `r ncol(AssociatedPress)` terms (distinct words)가 포함되어 있다. 이 DTM은 99% sparse (즉 문서와 단어 쌍의 99%는 0의 값을 가짐)이다. `Terms()`을 이용해서 문서 내의 term을 볼 수 있다.  

```{r terms, dependson="AssociatedPress"}
terms <- Terms(AssociatedPress)
head(terms)
```

tidy tools로 이 데이터를 분석하려면 먼저 one-token-per-document-per-row 형태의 데이터 프레임으로 변환해야 한다. `broom`package에 있는 `tidy()`를 사용하면 non-tidy object를 tidy data frame으로 변환할 수 있다.`tidytext`package는 `DocumentTermMatrix` objects에 대해 `tidy()`를 사용할 수 있게 되어 있다.  

```{r ap_td, dependson = "AssociatedPress"}
library(dplyr)
library(tidytext)

ap_td <- tidy(AssociatedPress)
ap_td
```

이제 `document`, `term`, `count`와 같이 3열을 가진 tidy format의 'tbl_df'를 얻을 수 있다. 이 변환은 sparse matrix가 아닌 matrix에 대한 `reshape2::melt()`와 유사한 결과를 출력한다.  

정리된 결과에는 0이 아닌 값만 출력된다.   document 1에는 "adding"이나 "adult"와 같은 용어가 포함되지만 "aaron" 또는 "abandon"은 포함되지 않는다. 이는 tidy version에 count = 0인 행은 포함되지 않았다는 것을 의미한다.  

이전 장에서 봤듯이 tidy format은 `dplyr`, `tidytext` 및 `ggplot2`package를 사용하기에 편리하다. 예를 들어 2장에 설명된 접근 방식을 이용하여 신문기사에 대한 감정분석을 수행할 수도 있다.  

```{r apsentiments, dependson = "ap_td"}
ap_sentiments <- ap_td %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))

ap_sentiments
```

이를 통해 AP신문기사에서 어떤 단어가 자주 긍정적으로 쓰였는지, 부정적으로 쓰였는지 시각화할 수 있다. (그림 5.2 참조)  
가장 흔하게 사용된 긍정적인 단어는 "like", "work", "support", "good" 등이고 가장 흔하게 사용된 부정적인 단어는 "killed", "death", "vice"(vice는 부통령- vice president -을 의미할 수도 있는데 이는 알고리즘의 실수로 보인다.) 등이 있는 것으로 확인된다.  

```{r apsentimentplot, dependson = "apsentiments", fig.height = 6, fig.width = 7, fig.cap = "Words from AP articles with the greatest contribution to positive or negative sentiments, using the Bing sentiment lexicon"}
library(ggplot2)

ap_sentiments %>%
  count(sentiment, term, wt = count) %>%
  ungroup() %>%
  filter(n >= 200) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(n, term, fill = sentiment)) +
  geom_col() +
  labs(x = "Contribution to sentiment", y = NULL)
```

### Tidying `dfm` objects

다른 텍스트 마이닝 패키지인 `quanteda`package에서 사용되는 `dfm` (document-feature matrix) class도 있다. 'dfm' class는 document-term matrix를 구현하는 또 다른 클래스이다.  예를 들어 `quanteda`package에는 대통령들의 취임 연설이 포함되어 있는데 이를 `token()`과 `dfm()`을 사용하여 'dfm' class로 변환 할 수 있다. 

```{r inaug_dfm, R.options=list(quanteda_print_dfm_max_ndoc = 0, quanteda_print_dfm_max_nfeat = 0)}
data("data_corpus_inaugural", package = "quanteda")
inaug_dfm <- data_corpus_inaugural %>%
  quanteda::tokens() %>%
  quanteda::dfm(verbose = FALSE)
inaug_dfm
```

`tidy` method는 이러한 dfm(document-feature matrices) 또한 one-token-per-document-per-row table로 변환해준다.  

```{r inaug_td, dependson = "inaug_dfm"}
inaug_td <- tidy(inaug_dfm)
inaug_td
```

각 취임 연설을 대표하는 중요 단어를 찾고자 한다면 3장에서 설명했던 `bind_tf_idf()`를 사용하여 각 단어-연설의 tf-idf를 계산하여 정량화 시킬 수 있다.  

```{r presidents, dependson = "inaug_td", fig.width = 8, fig.height = 8}
inaug_tf_idf <- inaug_td %>%
  bind_tf_idf(term, document, count) %>%
  arrange(desc(tf_idf))

inaug_tf_idf
```

이 데이터를 사용하여 링컨, 루즈벨트, 케네디, 오바마 대통령의 취임연설을 선택하여 그림 5.3과 같이 각 취임 연설을 대표할 수 있는 주요 단어를 시각화 할 수 있다.  

```{r presidentspeeches, dependson = "presidents", echo = FALSE, fig.cap = "The terms with the highest tf-idf from each of four selected inaugural addresses. Note that quanteda's tokenizer includes the '?' punctuation mark as a term, though the texts we've tokenized ourselves with `unnest_tokens()` do not.", fig.height=6, fig.width=7}
speeches <- c("1933-Roosevelt", "1861-Lincoln",
              "1961-Kennedy", "2009-Obama")

inaug_tf_idf %>%
  filter(document %in% speeches) %>%
  group_by(document) %>%
  slice_max(tf_idf, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  mutate(term = reorder_within(term, tf_idf, document)) %>%
  ggplot(aes(term, tf_idf, fill = document)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ document, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL,
       y = "tf-idf")
```

tidy data를 이용한 또 다른 시각화의 예로 각 문서의 이름에서 연도를 추출하고 각 연도별 총 단어 수를 계산해 볼 수도 있다.  


테이블에 0(단어가 문서에 쓰여지지 않는 경우)의 값을 가지는 경우를 포함하기 위해 `tidyr`package의 `complete()`를 사용했다.     

```{r year_term_counts, dependson = "inaug_td"}
library(tidyr)

year_term_counts <- inaug_td %>%
  extract(document, "year", "(\\d+)", convert = TRUE) %>%
  complete(year, term, fill = list(count = 0)) %>%
  group_by(year) %>%
  mutate(year_total = sum(count))

year_term_counts
```

이를 통해 그림 5.4에서 보는 바와 같이 시간이 지남에 따라 특정 단어의 사용빈도가 어떻게 변화했는지 시각화 할 수 있다. 시간이 지남에 따라 미국 대통령들은 나라를 연방(Union)으로 언급하는 빈도가 줄어들고 미국(America)으로 말하는 횟수가 증가했음을 알 수 있다. 또한 헌법(constitution)이나 외국(foreign)을 언급하는 횟수는 줄어드는 반면 자유(freedom)와 신(God)을 언급한 횟수가 증가했음을 알 수 있다.  

```{r yearterm, dependson = "year_term_counts", fig.cap = "Changes in word frequency over time within Presidential inaugural addresses, for six selected terms"}
year_term_counts %>%
  filter(term %in% c("god", "america", "foreign", "union", "constitution", "freedom")) %>%
  ggplot(aes(year, count / year_total)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~ term, scales = "free_y") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(y = "% frequency of word in inaugural address")
```

이러한 예시들을 통해 tidy format이 아닌 데이터를 변환하여 tidy tools과 `tidytext`package를 사용하여 분석하는 방법을 보여준다.  

## Casting tidy text data into a matrix {#cast-dtm}

몇몇 텍스트 마이닝 패키지들이 document-term matrices 형태의 샘플 데이터 제공하거나 output을 제공하는 것처럼 몇몇 알고리즘은 이런 형식의 matrices를 입력값으로 사용하기도 한다. 그래서 `tidytext`package는 tidy format을 이런 martrices 형태로 변환하기 위해 `cast_...()`를 제공한다.  

예를 들어 tidy format의 AP dataset을 `cast_dtm()`를 사용하여 document-term matrices 형태로 다시 변환 할 수 있다.  

```{r}
ap_td %>%
  cast_dtm(document, term, count)
```

비슷한 방식으로 `cast_dfm()`를 사용해서 `quanteda`package에서 사용되는 `dfm` object로 변환할 수도 있다.  

```{r chunk1, R.options=list(quanteda_print_dfm_max_ndoc = 0, quanteda_print_dfm_max_nfeat = 0)}
ap_td %>%
  cast_dfm(document, term, count)
```

sparse martix를 요구하는 몇몇 도구를 위해 `cast_sparse()`를 사용해 sparse matrix로 변환할 수도 있다.  

```{r}
library(Matrix)

# cast into a Matrix object
m <- ap_td %>%
  cast_sparse(document, term, count)

class(m)
dim(m)
```

이런 변환들은 지금까지 이 책에서 사용해왔던 tidy text format에서 쉽게 수행할 수 있다. 예를 들어 몇 줄의 코드로 제인 오스틴의 책을 DTM 형태로 만들 수 있다.  

```{r austen_dtm}
library(janeaustenr)

austen_dtm <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word) %>%
  cast_dtm(book, word, n)

austen_dtm
```

이러한 변환과정을 통해 dplyr 및 기타 tidy tools을 사용한 읽기, 필터링 등의 처리를 수행할 수 있다. 처리 후에 데이터를 머신러닝을 위한 document-term matrix로 변환할 수 있다. 6장에서는 tidy text dataset을 DocumentTermMatrix로 변환해야 하는 경우가 어떤 것이 있는지 알아볼 것이다.  

## Tidying corpus objects with metadata

일부 데이터 구조는 말뭉치(Corpus)라고 하는 토큰화되기 전 문서 모음을 저장하도록 되어 있다. 한 예로 `tm`package의 Corpus object를 들 수 있다. 이 object는 각 문서의 ID, 날짜/시간, 제목, 언어를 포함하는 **메타데이터**와 함께 텍스트를 저장한다.  

예를 들어 `tm`package에 포함된 `acq` corpus는 루이터의 신문기사 50개를 가지고 있는 데이터이다.  

```{r acq}
data("acq")
acq

# first document
acq[[1]]
```

corpus object는 list와 같은 구조로 구성되어 각 항목에는 텍스트와 메타데이터가 모두 포함된다. 이것은 문서를 저장하는데는 좋을지 몰라도 tidy tools로 처리하기에는 적합하지 않다.  

따라서 `tidy()` method를 사용해서 메타데이터들(`id`나 `datetimestamp` 같은)을 `text`열 옆에 열을 만들어 각 신문기사 행에 들어가도록 할 수 있다.  

```{r acq_td, dependson = "acq"}
acq_td <- tidy(acq)
acq_td
```

이렇게 변환한 다음 `unnest_tokens()`을 사용하여 50개의 루이터 기사에서 가장 빈도 수가 높은 단어를 찾거나 각 기사를 대표하는 특정 단어를 찾을 수 있다.  

```{r acq_tokens, dependson = "acq_td"}
acq_tokens <- acq_td %>%
  select(-places) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word")

# most common words
acq_tokens %>%
  count(word, sort = TRUE)

# tf-idf
acq_tokens %>%
  count(id, word) %>%
  bind_tf_idf(word, id, n) %>%
  arrange(desc(tf_idf))
```

### Example: mining financial articles {#financial}

`Corpus` objects는 데이터 수집 패키지에서 일반적으로 사용되는 출력 형식이므로 `tidy()`를 통해 다양한 텍스트 데이터를 들여다 볼 수 있다. 한 가지 예로 [tm.plugin.webmining](https://cran.r-project.org/package=tm.plugin.webmining)이 있는데 이는 키워드를 기반으로 뉴스 기사를 검색하기 위해 온라인 피드에 연결할 수 있게 해주는 패키지로 `WebCorpus(GoogleFinanceSource("NASDAQ:MSFT"))`와 같이 설정하면 Microsoft (MSFT) 주식과 관련된 최신 기사 20개를 검색할 수 있다.  

그래서 Microsoft, Apple, Google, Amazon, Meat(구.Facebook), Twitter, IBM, Yahoo, Netflix와 같은 주요 9개 기술주의 최근 기사를 검색해 본다. `tm.plugin.webmining`package가 java문제로 작동이 되지 않아 예전(2017) 기사 자료를 가져와 그대로 쓴다.    

```{r stock_articles_run, eval = FALSE}
library(tm.plugin.webmining)
library(purrr)

company <- c("Microsoft", "Apple", "Google", "Amazon", "Meta",
             "Twitter", "IBM", "Yahoo", "Netflix")
symbol  <- c("MSFT", "AAPL", "GOOG", "AMZN", "FB", 
             "TWTR", "IBM", "YHOO", "NFLX")

download_articles <- function(symbol) {
  WebCorpus(GoogleFinanceSource(paste0("NASDAQ:", symbol)))
}

stock_articles <- tibble(company = company,
                         symbol = symbol) %>%
  mutate(corpus = map(symbol, download_articles))
```

This uses the `map()` function from the purrr package, which applies a function to each item in `symbol` to create a list, which we store in the `corpus` list column.

```{r stock_articles, echo = FALSE}
load("data/stock_articles.rda")
```

```{r dependson = "stock_articles"}
stock_articles
```

`corpus` list 열의 각 항목은 `WebCorpus` object로 `acq`와 같이 특정 형태의 말뭉치이다. 따라서 `tidy()`를 사용해서 각각을 데이터 프레임으로 변환하고 `tidyr::unnest()`로 중첩된 내용을 해제한 다음 `unnest_tokens()`을 사용해 `text`열에 들어 있는 각 기사 내용을 토큰화 시킬 수 있다.  

```{r stock_tokens, dependson = "stock_articles"}
library(purrr)
stock_tokens <- stock_articles %>%
  mutate(corpus = map(corpus, tidy)) %>%
  unnest(cols = (corpus)) %>%
  unnest_tokens(word, text) %>%
  select(company, datetimestamp, word, id, heading)

stock_tokens
```

각 기사의 메타데이터 중 일부를 볼 수 있는데 tf-idf를 사용하여 각 기업 주식의 특징을 나타내는 단어를 찾을 수 있다.  

```{r stocktfidfdata, dependson="stock_tokens"}
library(stringr)

stock_tf_idf <- stock_tokens %>%
  count(company, word) %>%
  filter(!str_detect(word, "\\d+")) %>%
  bind_tf_idf(word, company, n) %>%
  arrange(-tf_idf)

stock_tf_idf
```

각 기업에 대한 주요 단어는 그림 5.5에 시각화 되어있다. 예상대로 회사의 이름과 티커가 포함되어 있지만 그들과 거래 중인 기업(디즈니와 넷플릭스)이나 경영진, 일부 제품명이 포함되어 있다.  

```{r stocktfidf, dependson = "stocktfidfdata", echo = FALSE, fig.cap = "The 8 words with the highest tf-idf in recent articles specific to each company", fig.height = 8, fig.width = 8}
stock_tf_idf %>%
  group_by(company) %>%
  top_n(8, tf_idf) %>%
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = company)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ company, scales = "free") +
  coord_flip() +
  labs(x = NULL,
       y = "tf-idf")
```

최근 뉴스를 사용하여 시장을 분석하고 투자 결정을 내리는데 관심이 있다면 뉴스 기사가 긍정적인지 부정적인지 판단하기 위해 감정분석을 사용하고 싶을 것이다. 이럴 때는 2.4장에서 본 것처럼 어떤 단어가 긍정적인 감정 혹은 부정적인 감정에 가장 많이 기여하는지 찾아봐야 한다. 그림 5.6과 같이 AFINN lexicon을 이용하여 찾아본다.  

```{r eval=FALSE}
stock_tokens %>%
  anti_join(stop_words, by = "word") %>%
  count(word, id, sort = TRUE) %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  summarize(contribution = sum(n * value)) %>%
  slice_max(abs(contribution), n = 12) %>%
  mutate(word = reorder(word, contribution)) %>%
  ggplot(aes(contribution, word)) +
  geom_col() +
  labs(x = "Frequency of word * AFINN value", y = NULL)
```


```{r stockafinn, dependson = "stock_articles", echo=FALSE, fig.cap = "The words with the largest contribution to sentiment values in recent financial articles, according to the AFINN dictionary. The 'contribution' is the product of the word and the sentiment score."}
load("data/afinn.rda")
stock_tokens %>%
  anti_join(stop_words, by = "word") %>%
  count(word, id, sort = TRUE) %>%
  inner_join(afinn, by = "word") %>%
  group_by(word) %>%
  summarize(contribution = sum(n * value)) %>%
  slice_max(abs(contribution), n = 12) %>%
  mutate(word = reorder(word, contribution)) %>%
  ggplot(aes(contribution, word)) +
  geom_col() +
  labs(x = "Frequency of word * AFINN value", y = NULL)
```

이런 금융 관련 기사를 분석할 때 주의해야 할 점이 있는데 예를 들어 "share"라는 단어는 AFINN lexicon에서는 나눈다는 뜻의 긍정적인 동사로("Alice will **share** her cake with Bob") 간주되지만 실제로는 주식("The stock price is $12 per **share**")이라는 뜻의 중립적인 의미의 명사이다. "fool"이라는 단어는 더 판단이 어렵게 쓰인다. 금융기사에서 자주 나오는 "fool"은 금융 서비스 회사인 "Motley Fool"에서 가져온 경우가 많다. 간단히 말해 AFINN lexicon은 금융 데이터의 맥락을 분석할 때는 (NRC나 Bing도 마찬가지지만) 적합하지 않다는 것을 알 수 있다.  

대신 Loughran과 McDonald사전의 financial sentiment terms [@loughran2011liability]을 사용해 볼 수 있다. 이 사전은 재무 보고서 분석을 목적으로 개발되었기 때문에 "share", "fool"과 같은 단어는 물론 재무적인 측면에서 부정적으로 볼 필요가 없는 "liability" 나 "risk"와 같은 용어를 적절하게 처리할 수 있다.  

Loughran data 는 단어를 6가지 감정- "positive(긍정적인)", "negative(부정적인)", "litigious(소송적인)", "uncertain(불확실한)", "constraining(제약적인)", "superfluous(불필요한)"로 나눈다. 이 텍스트 데이터 셋 내에서 각 감정에 속하는 가장 빈도가 높은 단어를 조사하는 것으로 시작해 본다.   

```{r eval=FALSE}
stock_tokens %>%
  count(word) %>%
  inner_join(get_sentiments("loughran"), by = "word") %>%
  group_by(sentiment) %>%
  slice_max(n, n = 5, with_ties = FALSE) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") +
  labs(x = "Frequency of this word in the recent financial articles", y = NULL)
```


```{r stockloughransentiments, echo = FALSE, fig.cap = "The most common words in the financial news articles associated with each of the six sentiments in the Loughran and McDonald lexicon"}
load("data/loughran.rda")
stock_tokens %>%
  count(word) %>%
  inner_join(loughran, by = "word") %>%
  group_by(sentiment) %>%
  slice_max(n, n = 5, with_ties = FALSE) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") +
  labs(x = "Frequency of this word in the recent financial articles", y = NULL)
```

단어에 대해 감정을 이런 식(그림 5.7 참조)으로 나누는 것이 더 합리적으로 보인다. 일반적으로 "strong"이나 "better"은 긍정적인 단어로 보고, "shares" 혹은 "growth"는 포함되지 않는 반면 "volatility"은 부정적인 단어로 인식하고 "fool"은 부정적으로 인식하지 않는다. 다른 감정 분석도 합리적으로 보이는데 "could"와 "may"는 "uncertainty"에 포함되는 것으로 나와있다. 

기사의 감정분석을 추정하는데 사용할 적합한 사전을 찾았으니 각 말뭉치에서 각 감정 관련 단어의 사용횟수를 계산해 본다.  

```{r eval=FALSE}
stock_sentiment_count <- stock_tokens %>%
  inner_join(get_sentiments("loughran"), by = "word") %>%
  count(sentiment, company) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)

stock_sentiment_count
```

```{r echo=FALSE}
stock_sentiment_count <- stock_tokens %>%
  inner_join(loughran, by = "word") %>%
  count(sentiment, company) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)

stock_sentiment_count
```

어떤 회사가 "litigious" 혹은 "uncertain"의 의미를 가진 단어가 쓰여진 뉴스에서 가장 많이 언급되었는지 조사해보자.  가장 간단한 방법은 2장에서 해 본 것처럼 뉴스가 긍정적인지 부정적인지 확인해 보는 것이다. 감정을 정량적으로 표현하기 위해서 "(positive - negative) / (positive + negative)" 값을 사용하여 시각화 할 것이다. (그림 5.8 참조)  

```{r stockpositivity, fig.cap = "\"Positivity\" of the news coverage around each stock in January 2017, calculated as (positive - negative) / (positive + negative), based on uses of positive and negative words in 20 recent news articles about each company"}
stock_sentiment_count %>%
  mutate(score = (positive - negative) / (positive + negative)) %>%
  mutate(company = reorder(company, score)) %>%
  ggplot(aes(score, company, fill = score > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Positivity score among 20 recent news articles", y = NULL)
```

분석 결과를 보면 2017년 1월 뉴스에서 트위터와 야후에 관련된 기사는 부정적인 내용이 실렸고, 구글과 아마존에 대해서는 긍정적인 기사가 많이 쓰여진 것을 확인할 수 있다. 추가 분석에 관심이 있다면 R에서 사용가능한 많은 금융 분석 패키지를 사용하여 최근 주가 및 여러 측정 항목들과 기사 분석 결과를 비교해 볼 수 있다.  

## Summary

텍스트 분석은 다양한 도구를 사용해야 하며 그 중 많은 도구들은 tidy format이 아닌 입/출력을 사용한다. 이번 챕터에서는 tidy text data frame과 sparse document-term matrices간 변환 방법, 문서의 메타데이터가 포함된 Corpus object를 정리하는 방법을 보여주었다. 다음 장에서는 document-term matrix를 입력으로 사용하는 `topicmodel`package를 사용하는 예를 보여주고 이러한 변화 도구가 텍스트 분석의 필수적인 부분임을 보여줄 것이다.  